{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba78d15",
   "metadata": {},
   "source": [
    "# Script para gestionar el procesadoo de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645c0c97",
   "metadata": {},
   "source": [
    "El objetivo es crear un script capaz de gestionar los archivos csv que se coloquen en una carpeta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d79b73",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "642d646f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para gestionar el directorio\n",
    "import os\n",
    "import time\n",
    "\n",
    "#Para filtrar los datos\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65835817",
   "metadata": {},
   "source": [
    "## Definición de variables\n",
    "En este primer apartado se pueden definir algunas de las variables que condicionarán el resultado final del programa. Aquí lo que hace cada una de ellas:\n",
    "* En el inicio:\n",
    "    * Espera: Variable booleana para esperar tras la verificación del arbol de trabajo (para que se pueda ingresar los ficheros sin necesidad de reiniciar el programa) o si por el contrario no se quiere hacer la pausa (por motivos de fluidez).\n",
    "* En la parte de extracción de ficheros:\n",
    "    * Lista_exclusiones: Es una lista con los nombres de las carpetas que no se quieren añadir al procesado. Por ejemplo, si dentro de la lista está el nombre \"S7\" en el caso de encontrar una carpeta así llamada en alguno de los directorios su contenido no se procesará.\n",
    "    * max_directorios: Indica el número máximos de directorios que se deben de tener en cuenta. La principal función de dicha variable es dar una opción de escape a un bucle while. Si hay menos carpetas que las indicadas no pasa nada, pero si hay más llegado al nivel indicado el programa parará (y por lo tanto los ficheros que esten dentro de las carpetas más abajo no se procesarán).\n",
    "* En la parte de obtención de la matriz:\n",
    "    * junto_X: Variable booleana que especifica si quieres obtener las etiquetes (en caso de haberlas) en la misma matriz de salida o si las quieres en un fichero aparte.\n",
    "    * add_timestamp: Añade a la matriz de salida una columna extra con la etiqueta de tiempo de los datos originales.\n",
    "    * borrar_datos_nuevos: Variable booleana que indica si quieres borrar los datos referentes a AP's solo vistos en los datos de Testeo y/o Validación o si los quieres conservar (en caso afirmativo aparecerán al final de la lista de AP's base).\n",
    "    * inv_value: Valor al que se pondrán los puntos de accesos que no se hayan visto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b07b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para el inicio\n",
    "Espera = False\n",
    "\n",
    "#Para la extracción de ficheros\n",
    "#Lista con los nombre de los directorios que se quieran excluir\n",
    "Lista_exclusiones=[\n",
    "    \"S7\"\n",
    "]\n",
    "\n",
    "#Número máximo de directorios que puede abrir antes de parar\n",
    "max_directorios = 5\n",
    "\n",
    "#Variable para indicar si se busca ordenar las lista de APs al crear la matriz o si se forman conforma vayan saliendo\n",
    "ordenar_listas=False #False= bucle for, True=np.uniques\n",
    "\n",
    "#Para la obtención de las matrices\n",
    "#Definimos si queremos las etiquetas en la misma matriz que los datos o por separado y si queremos borrar los datos que no aparezcan en la lista\n",
    "junto_Train = True\n",
    "junto_Test = True\n",
    "junto_Val = True\n",
    "\n",
    "#Para añadir la columna de tiempo a la matriz de salida\n",
    "add_timestamp = True\n",
    "\n",
    "#Definimos si queremos eliminar o conservar los datos que hagan referencia a AP's que solo se encuentren en los ficheros de validación y testeo\n",
    "borrar_datos_nuevos_Test = True\n",
    "borrar_datos_nuevos_Val = True\n",
    "\n",
    "#Variable para lanzar el checkeo del valor mínimo\n",
    "check_minimun = True\n",
    "\n",
    "#Valor por el que se reempplazarán las potencias que no se vean\n",
    "inv_value=-100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775b7f0",
   "metadata": {},
   "source": [
    "## Creación del espacio de trabajo\n",
    "Esta parte del código se encargará de crear las diferentes carpetas en las que se almacenarán los datos procesados. Para ello lo primero que haremos en verificar si ya existe la configuración adecuada, y de no ser así se creará, indicando al usuario como ha de proceder.\n",
    "La idea es que el arbol de trabajo sea el siguiente:\n",
    "\n",
    "    |->Database\n",
    "        |->Raw_data\n",
    "            |->Train\n",
    "            |->Test\n",
    "            |->Val\n",
    "        |->Processed_data\n",
    "            |->Dia\n",
    "                |->Hora\n",
    "            \n",
    "En la carpeta \"Raw_data\" es donde irían los .csv que se van a procesar. Dentro de la misma hay varias opciones a la hora de procesar los datos:\n",
    "* Si se añaden csv en las carpetas \"Train\", \"Test\" y \"Val\" esos datos se usarán para dicho proceso.\n",
    "* Si se añaden listas que contengan \"listado\" en el nombre a alguna de las carpetas los datos de ese conjunto se procesaran siguiendo dicho listado.\n",
    "\n",
    "Finalmente los datos procesados se pueden recoger en la carpeta \"Processed_data\". Para evitar que se sobreescriban los datos se crea una carpeta cada vez que se lanza el programa, en la cual se indica el día (carpeta general) y la hora (subcarpeta en la que se guardan los datos procesados).\n",
    "\n",
    "En la carpeta de datos procesados siempre encontraras uno o varios archivos .csv (dependiendo de cuantos conjuntos vayas a crear y de si quieres las etiquetes juntas o separadas), junto con el listado de los AP's únicos que se ha usado para procesarlos y un .txt con información diversa del proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdcdaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para crear las carpetas a partir de una lista de direcciones\n",
    "def Crea_directorios(lista):\n",
    "  for direccion in lista:\n",
    "    os.mkdir(direccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b49ec653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrada la carpeta 'Database'. Procedemos a verificar que es la adecuada.\n",
      "\u001b[1mCarpeta identificada con éxito.\u001b[0m\n",
      "\u001b[1m[info]\u001b[0m: Por favor, diríjase a la dirección: '/home/laura/Adrian/Procesaor_dataset/Database' e ingrese los archivos .csv en la carpeta 'Raw_data' para continuar.\n",
      "Dentro de esa carpeta encontrará varias opciones, coloque los .csv en las carpetas de las que quiera crear un conjunto.\n",
      "Por ejemplo, si ingresa 2 archivos en la carpeta 'Train' ambos se procesaran como datos de entrenamiento, y si además mete otro dos en la carpeta 'Test' estos archivos se procesaran aparte en un conjunto de testeo.\n",
      "\u001b[1m[info]\u001b[0m: También puede meter un listado base con los AP's conforme los quieras colocar. Los datos se procesaran teniendo en cuenta esa lista.\n",
      "\u001b[1m[importante]\u001b[0m: Si quieres meter un listado en alguna carpeta asegurate de que este contenga el nombre 'listado'.\n",
      "\u001b[1m[info]\u001b[0m: Los archivos que queden fuera de alguna de estas carpetas no seran procesados.\n"
     ]
    }
   ],
   "source": [
    "#Definimos todas las direcciones necesarias.\n",
    "current_path = os.getcwd()\n",
    "external_path = current_path + \"/Database\"\n",
    "\n",
    "raw_path = external_path + \"/Raw_data\"\n",
    "raw_train_path = raw_path + \"/Train\"\n",
    "raw_test_path = raw_path + \"/Test\"\n",
    "raw_val_path = raw_path + \"/Val\"\n",
    "\n",
    "#Y las direcciones de los archivos de salida\n",
    "processed_path = external_path + \"/Processed_data\"\n",
    "str_date = str(time.gmtime().tm_mday)+\"_\"+str(time.gmtime().tm_mon)+\"_\"+str(time.gmtime().tm_year)\n",
    "str_hour = str(time.gmtime().tm_hour)+\":\"+str(time.gmtime().tm_min)+\":\"+str(time.gmtime().tm_sec)\n",
    "date_path = processed_path + \"/\" + str_date\n",
    "hour_path = date_path + \"/\" + str_hour\n",
    "\n",
    "#Para la informacion\n",
    "str_info = (\"Información sobre el procesado de datos ejecutado el día \" + str(str_date) + \" a las \" +str(str_hour) +\".\\n\")\n",
    "\n",
    "lista_direcciones=[external_path,raw_path,raw_train_path,raw_test_path,raw_val_path,processed_path]\n",
    "\n",
    "#Primero comprobamos si existe la carpeta adecuada. \n",
    "if(os.path.exists(external_path)):\n",
    "    print(\"Encontrada la carpeta 'Database'. Procedemos a verificar que es la adecuada.\")\n",
    "    if(os.path.exists(raw_path) & os.path.exists(processed_path)):\n",
    "        if(os.path.exists(raw_train_path) & os.path.exists(raw_test_path) & os.path.exists(raw_val_path)):\n",
    "            print('\\033[1mCarpeta identificada con éxito.\\033[0m')\n",
    "        else:\n",
    "            print(\"Parece que hay un error. El arbol de trabajo es incorrecto, lo cual podría indicar que la carpeta 'Database' fue creada con otro fin. Procedo a cambiarla el nombre a 'Database_antigua' y creo un nuevo directorio con la configuración adecuada.\")\n",
    "            os.rename(external_path, external_path+'_antigua_' + str(time.time()))\n",
    "            Crea_directorios(lista_direcciones)\n",
    "            \n",
    "    else:\n",
    "        print(\"Parece que hay un error. El arbol de trabajo es incorrecto, lo cual podría indicar que la carpeta 'Database' fue creada con otro fin. Procedo a cambiarla el nombre a 'Database_antigua' y creo un nuevo directorio con la configuración adecuada.\")\n",
    "        os.rename(external_path, external_path+'_antigua_' + str(time.time()))\n",
    "        Crea_directorios(lista_direcciones)\n",
    "\n",
    "else:\n",
    "    print(\"No se ha encontrado la carpeta 'Database'. Se procede a crear todos los directorios.\")\n",
    "    Crea_directorios(lista_direcciones)    \n",
    "    print(\"El arbol de trabajo ya ha sido creado.\")\n",
    "    \n",
    "print(\"\\033[1m[info]\\033[0m: Por favor, diríjase a la dirección: '\"+ str(external_path) +\"' e ingrese los archivos .csv en la carpeta 'Raw_data' para continuar.\")\n",
    "print(\"Dentro de esa carpeta encontrará varias opciones, coloque los .csv en las carpetas de las que quiera crear un conjunto.\")\n",
    "print(\"Por ejemplo, si ingresa 2 archivos en la carpeta 'Train' ambos se procesaran como datos de entrenamiento, y si además mete otro dos en la carpeta 'Test' estos archivos se procesaran aparte en un conjunto de testeo.\")\n",
    "print(\"\\033[1m[info]\\033[0m: También puede meter un listado base con los AP's conforme los quieras colocar. Los datos se procesaran teniendo en cuenta esa lista.\")\n",
    "print(\"\\033[1m[importante]\\033[0m: Si quieres meter un listado en alguna carpeta asegurate de que este contenga el nombre 'listado'.\")\n",
    "print(\"\\033[1m[info]\\033[0m: Los archivos que queden fuera de alguna de estas carpetas no seran procesados.\")\n",
    "\n",
    "if(Espera):\n",
    "    #Creamos una espera por si no se han metido los datos\n",
    "    input(\"Cuando tengas todo listo pulsa el botón \\033[1m'Enter'\\033[0m y procederemos con el procesado de los datos.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72e133a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empezamos a contar para saber cuanto tardamos en ejecutar el programa\n",
    "tiempo_inicio = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4a8b7a",
   "metadata": {},
   "source": [
    "## Obtención de los datos\n",
    "En esta parte del código  trabajaremos en los archivos .csv que se encuentren en la carpeta \"Raw_data\". La idea es que el código lea todos los archivos que encuentre y los procese, independientemente de la cantidad, por lo que el usuario es libre de meter cuantos archivos quiera."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2370203b",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3766b5",
   "metadata": {},
   "source": [
    "Primero comprobamos que haya algún dato a procesar en alguna de las carpetas, y de no ser así avisamos al usuario para que los meta. \n",
    "Dejamos listadas las ubicaciones para facilitar su procesado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18884d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para diferenciar los ficheros y carpetas de una lista de direcciones \n",
    "def Encuentra_ficheros(lista_direcciones):\n",
    "    #La función ha de generar una lista con las direcciones de los ficheros a apartir de una dirección de un nivel superior\n",
    "    Lista_ficheros = []\n",
    "    Lista_directorios = []\n",
    "\n",
    "    for direccion in lista_direcciones:\n",
    "        #print(direccion)\n",
    "    \n",
    "        if os.path.isdir(direccion): \n",
    "            Lista_directorios.append(direccion)\n",
    "            #print(\"Encontrado directorio.\")\n",
    "\n",
    "        else:\n",
    "            Lista_ficheros.append(direccion)\n",
    "    \n",
    "    return Lista_ficheros, Lista_directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d44f3bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han encontrado los siguientes archivos en la carpeta '\u001b[1mTrain\u001b[0m':\n",
      "\t ⏺Carpeta: Mes07-20072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-20072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-20072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-22072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-22072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-22072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-19072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-19072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-19072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes06-30062021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes06-30062021 se encuentra la sub-carpeta Nexus que contenía 7 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes06-30062021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-09072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-09072021 se encuentra la sub-carpeta Nexus que contenía 6 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-09072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-29072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-29072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-29072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-12072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-12072021 se encuentra la sub-carpeta Nexus que contenía 11 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-12072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-08072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-08072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-08072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-12082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-12082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-12082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-05082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-05082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-05082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-06072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-06072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-06072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-15072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-15072021 se encuentra la sub-carpeta Nexus que contenía 7 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-15072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-16072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-16072021 se encuentra la sub-carpeta Nexus que contenía 7 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-16072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-10082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-10082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-10082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-04082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-04082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-04082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-14072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-14072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-14072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-27072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-27072021 se encuentra la sub-carpeta Nexus que contenía 5 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-27072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes06-29062021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes06-29062021 se encuentra la sub-carpeta Nexus que contenía 9 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes06-29062021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-28072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-28072021 se encuentra la sub-carpeta Nexus que contenía 6 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-28072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-07072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-07072021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-07072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes07-05072021 formada por 0 ficheros y 1 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-05072021 se encuentra la sub-carpeta Nexus que contenía 5 elementos.\n",
      "\t ⏺Carpeta: Mes07-23072021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes07-23072021 se encuentra la sub-carpeta Nexus que contenía 7 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes07-23072021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "Se han encontrado los siguientes archivos en la carpeta '\u001b[1mTest\u001b[0m':\n",
      "\t ⏺Carpeta: Mes09-10092021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes09-10092021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes09-10092021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "Se han encontrado los siguientes archivos en la carpeta '\u001b[1mVal\u001b[0m':\n",
      "\t ⏺Carpeta: Mes08-23082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-23082021 se encuentra la sub-carpeta Nexus que contenía 7 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-23082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-18082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-18082021 se encuentra la sub-carpeta Nexus que contenía 6 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-18082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-24082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-24082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-24082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-16082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-16082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-16082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n",
      "\t ⏺Carpeta: Mes08-26082021 formada por 0 ficheros y 2 subcarpetas.\n",
      "\t\t ⏺Dentro de la carpeta Mes08-26082021 se encuentra la sub-carpeta Nexus que contenía 8 elementos.\n",
      "\t\t ⏺[Exclusión]: Dentro de la carpeta Mes08-26082021 se encuentra la sub-carpeta S7 que pertenece a la lista de exclusión, por lo que no será procesada.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Lista_raw_path=[\n",
    "    \"raw_train_path\",\n",
    "    \"raw_test_path\",\n",
    "    \"raw_val_path\"\n",
    "]\n",
    "\n",
    "str_info = str_info + \"\\n\\u25BA Registro de ficheros \\u25C4\\n\"\n",
    "\n",
    "#Recorremos la lista de direcciones \n",
    "for path in Lista_raw_path:\n",
    "    #Guardamos el nombre del conjunto (Train, Test, etc)\n",
    "    conjunto =  path[4].upper() + path[5:-5]\n",
    "    \n",
    "    #Si está vacio no hacemos nada, pero llevamos la cuenta para asegurar de que al menos una carpeta contenga algo\n",
    "    if(len(os.listdir(globals()[\"%s\" % path]))==0):\n",
    "        print(\"La carpeta '\\033[1m\"+str(conjunto)+\"\\033[0m' esta vacia.\")\n",
    "        \n",
    "    #Si tiene algo en su interior analizamos que clase de elemento es\n",
    "    else:\n",
    "        print(\"Se han encontrado los siguientes archivos en la carpeta '\\033[1m\"+str(conjunto)+\"\\033[0m':\")\n",
    "        str_info = str_info +\"Se han extraido datos de los siguientes archivos localizados en la carpeta \"+str(conjunto)+\":\\n\"\n",
    "        \n",
    "        #Inicializamos la matriz donde almacenaremos las direcciones de los archivos\n",
    "        globals()['direcciones_%s' % conjunto]=[]\n",
    "        \n",
    "        for elemento in os.listdir(globals()[\"%s\" % path]):\n",
    "            \n",
    "            #Nos aseguramos que el elemento no pertenezca a la lista de exclusiones\n",
    "            if(elemento not in Lista_exclusiones):\n",
    "                path_elemento = globals()[\"%s\" % path] + \"/\" + elemento\n",
    "\n",
    "                #Si el elemento es un fichero\n",
    "                if os.path.isdir(path_elemento)== False:\n",
    "                    globals()['direcciones_%s' % conjunto] += [path_elemento]\n",
    "\n",
    "                    print(\"\\t \\u23FADocumento: \" +str(elemento))\n",
    "                    str_info = str_info + \"\\t \\u23FADocumento: \" +str(elemento) +\"\\n\"\n",
    "\n",
    "                else:\n",
    "                    #Primero buscamos los ficheros desde el nivel superior\n",
    "                    Lista_superior = [path_elemento +\"/\" + files for files in os.listdir(path_elemento)]\n",
    "                    Lista_ficheros, Lista_directorios = Encuentra_ficheros(Lista_superior)\n",
    "                    #print(\"\\033[1mFicheros:\\033[0m \" + str(Lista_ficheros))\n",
    "                    #print(\"\\033[1mDirectorios:\\033[0m \" + str(Lista_directorios))\n",
    "\n",
    "                    print(\"\\t \\u23FACarpeta: \" +str(elemento) +\" formada por \"+str(len(Lista_ficheros))+\" ficheros y \"+str(len(Lista_directorios))+\" subcarpetas.\")\n",
    "                    str_info=str_info + \"\\t \\u23FACarpeta: \" +str(elemento) +\" formada por \"+str(len(Lista_ficheros))+\" ficheros y \"+str(len(Lista_directorios))+\" subcarpetas.\\n\"\n",
    "                    \n",
    "                    #En caso de encontrarnos con directorios en los niveles inferiores vamos bajando hasta sacar todos los ficheros\n",
    "                    contador = 0\n",
    "                    start_str = \"\\t \\u23FA\"\n",
    "                    str_sub=\"\"\n",
    "                    while((len(Lista_directorios)!=0) and (contador <= max_directorios)):\n",
    "                        \n",
    "                        #print(\"'\\033[1mEntrando en el bucle while'\\033[0m\")\n",
    "                        start_str = \"\\t\"+start_str\n",
    "                        str_sub = \"sub-\"+str_sub\n",
    "                        lis_direc=[]\n",
    "                        lis_fic=[]\n",
    "\n",
    "                        for direc in Lista_directorios:\n",
    "                            \n",
    "                            if(str(direc.split(\"/\")[-1]) not in Lista_exclusiones):\n",
    "                                print(start_str + \"Dentro de la carpeta \"+str(direc.split(\"/\")[-2])+\" se encuentra la \"+str_sub+\"carpeta \"+str(direc.split(\"/\")[-1])+\" que contenía \"+str(len(os.listdir(direc)))+\" elementos.\")\n",
    "                                str_info=str_info + start_str + \"Dentro de la carpeta \"+str(direc.split(\"/\")[-2])+\" se encuentra la \"+str_sub+\"carpeta \"+str(direc.split(\"/\")[-1])+\" que contenía \"+str(len(os.listdir(direc)))+\" elementos.\\n\"\n",
    "                                \n",
    "                                lis_direc+=[direc + \"/\" + str(fichero) for fichero in os.listdir(direc)]\n",
    "                            \n",
    "                            else:\n",
    "                                print(start_str +\"[Exclusión]: Dentro de la carpeta \"+str(direc.split(\"/\")[-2])+\" se encuentra la \"+str_sub+\"carpeta \"+str(direc.split(\"/\")[-1])+\" que pertenece a la lista de exclusión, por lo que no será procesada.\\n\")\n",
    "                                str_info=str_info + start_str + \"[Exclusión]: Dentro de la carpeta \"+str(direc.split(\"/\")[-2])+\" se encuentra la \"+str_sub+\"carpeta \"+str(direc.split(\"/\")[-1])+\" que pertenece a la lista de exclusión, por lo que no será procesada.\\n\"\n",
    "                        \n",
    "                        lis_fic, Lista_directorios = Encuentra_ficheros(lis_direc)\n",
    "                        if(len(lis_fic)!=0):\n",
    "                            Lista_ficheros = Lista_ficheros + lis_fic\n",
    "\n",
    "                        contador = contador +1\n",
    "\n",
    "                    globals()['direcciones_%s' % conjunto] += Lista_ficheros\n",
    "                    #print(\"\\t En total, dentro de \"+str(elemento)+\" se han localizado \"+str(len(Lista_ficheros))+\" archivos.\")\n",
    "                    #str_info=str_info+\"\\t En total, dentro de \"+str(elemento)+\" se han localizado \"+str(len(Lista_ficheros))+\" archivos.\\n\"\n",
    "\n",
    "                    #print(\"\\033[1mFicheros:\\033[0m \" + str(Lista_ficheros)+\"\\n\\033[1mTamaño:\\033[0m \"+str(len(Lista_ficheros)))\n",
    "                    #print(\"\\033[1mDirectorios:\\033[0m \" + str(Lista_directorios))\n",
    "                    \n",
    "            else:\n",
    "                print(\"\\t \\u23FAEl elemento \"+ str(element)+\" se encuentra dentro de la lista de exclusiones, por lo que no se procesará.\")\n",
    "                str_info=str_info +\"\\t \\u23FAEl elemento \"+ str(element)+\" se encuentra dentro de la lista de exclusiones, por lo que no se procesará.\\n\"\n",
    "           \n",
    "    if('direcciones_'+ str(conjunto) in globals()):\n",
    "        globals()['direcciones_%s' % conjunto] = sorted(globals()['direcciones_%s' % conjunto])\n",
    "    #print(\"\\033[1mFicheros \" +str(conjunto)+ \":\\033[0m \" + str(globals()['direcciones_%s' % conjunto]))\n",
    "    #print(\"\\033[1mDirectorios \" +str(conjunto)+ \":\\033[0m \" + str(Lista_directorios))\n",
    "    #print(\"\\n\")\n",
    "    \n",
    "assert ((\"direcciones_Train\" in globals()) or (\"direcciones_Test\" in globals()) or (\"direcciones_Val\" in globals())), \"[\\033[1mImportante\\033[0m]: No se ha encontrado ninguna archivo que procesar. Por favor, introduzca algún archivo y verifique que este no pertenezca a la lista de exclusiones.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4756ec",
   "metadata": {},
   "source": [
    "### Función para sacar las matrices\n",
    "\n",
    "Una vez tenemos listadas las direcciones de todos los archivos que vamos a procesar, creamos una función que tendrá como entrada ese listado y como salida una matriz con todos datos.\n",
    "La variable \"secuencia\" cuenta con las muestras que tiene cada fichero csv, de forma que acaba siendo una lista donde se guardan todas las secuencias que se han procesado.\n",
    "También en el caso de que exista un fichero \"listado\" en alguna de las carpetas lo procesará para que se puedan ordenar los datos conforme allí aparezcan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754a4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Saca_matrices(direcciones):\n",
    "    #Almacenaremos los datos en una lista de listas de tamaño variable en función de la cantidad de ficheros que haya\n",
    "    datos_totales=[]\n",
    "    secuencias=[]\n",
    "    listado = None\n",
    "    orden=[]\n",
    "    \n",
    "    #Para cargar los datos usamos pd.read_csv(), el cual nos carga los datos en formato Dataframe, pero nosotros lo convertiremos a lista para poder trabajar con ello\n",
    "    for direccion in direcciones:\n",
    "        #Comprobamos que no sea un archivo de listado\n",
    "        if(\"listado\" in direccion):\n",
    "            listado = pd.read_csv(direccion, header = None).to_numpy()[1:]\n",
    "            listado = np.array([item for sublist in listado for item in sublist])\n",
    "            print(\"[Importante]: Se ha encontrado una lista base\")\n",
    "            globals()[\"str_info\"]=globals()[\"str_info\"] + \"[Importante]: Se ha encontrado una lista base\\n\"\n",
    "        else:\n",
    "            print(direccion)\n",
    "            datos_totales.append((pd.read_csv(direccion, on_bad_lines='skip', header = None)).to_numpy().tolist())\n",
    "    \n",
    "    #Mostramos la cantidad de datos que se han leido para asegurarnos más tarde de que no se pierda ninguno\n",
    "    print(\"En total se han descargado \"+ str(len(datos_totales)) +\" ficheros, los cuales se colocarán siguiendo el orden que se muestra a continuación:\")\n",
    "    globals()[\"str_info\"]=globals()[\"str_info\"] + \"En total se han descargado \"+ str(len(datos_totales)) +\" ficheros, los cuales tienen las siguientes dimensiones:\\n\"\n",
    "    \n",
    "    cuenta_datos = 0\n",
    "    for i in range(len(datos_totales)):\n",
    "        print(\"El archivo '\"+ str(direcciones[i]) +\" contenía \"+ str(len(datos_totales[i])) +\" datos.\")\n",
    "        print(\"En total representaban \"+str(datos_totales[i][-1][0])+\" secuencias.\")\n",
    "        globals()[\"str_info\"]=globals()[\"str_info\"] + \"\\t\\u23FA\" + \"El archivo '\"+ str(direcciones[i]) +\" contenía \"+ str(len(datos_totales[i])) +\" datos, los cuales en total representaban \"+str(datos_totales[i][-1][0] +1)+\" secuencias.\\n\"\n",
    "        cuenta_datos = cuenta_datos + len(datos_totales[i])\n",
    "        secuencias.append(datos_totales[i][-1][0])\n",
    "        orden.append([str(direcciones[i]), datos_totales[i][-1][0]])\n",
    "        \n",
    "    print(\"Por lo que el total de datos a procesar tiene que ser de \"+str(cuenta_datos))\n",
    "    \n",
    "    #Una vez cargados los datos los pasaremos de una lista de listas a una sola lista\n",
    "    flat_list = [item for sublist in datos_totales for item in sublist]\n",
    "    print(\"Al realizar el 'aplanamiento' nos quedamos con un total de \"+ str(len(flat_list)))\n",
    "    assert len(flat_list) == cuenta_datos, \"Ha surgido un error al aplanar los datos. Originalmente había \"+ str(cuenta_datos) +\", pero tras aplanar nos hemos quedado con \"+ str(len(flat_list)) +\".Por favor, revisa el código\"\n",
    "    \n",
    "    #Escribimos más informacion\n",
    "    globals()[\"str_info\"]=globals()[\"str_info\"] + \"El total de datos a procesar dentro de este conjunto ha de ser de \"+str(cuenta_datos)+ \" contenidos en \"+str(sum(secuencias)+len(secuencias))+\" secuencias.\\n\"\n",
    "    \n",
    "    #Finalmente convertimos dicha lista a formato matriz para poder trabajar con ella de manera cómoda\n",
    "    matriz = np.array(flat_list)\n",
    "    \n",
    "    return matriz, secuencias, listado, orden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e11078b",
   "metadata": {},
   "source": [
    "Y pasamos por la función todas las listas que hayamos creado anteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2de11e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSet de entrenamiento\u001b[0m\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t9_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t10_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t11_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t9_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t4_Nexus.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t8_Nexus.csv\n",
      "En total se han descargado 166 ficheros, los cuales se colocarán siguiendo el orden que se muestra a continuación:\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t1_Nexus.csv contenía 4423 datos.\n",
      "En total representaban 128 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t2_Nexus.csv contenía 3838 datos.\n",
      "En total representaban 134 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t3_Nexus.csv contenía 2978 datos.\n",
      "En total representaban 107 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t4_Nexus.csv contenía 4445 datos.\n",
      "En total representaban 143 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t5_Nexus.csv contenía 4988 datos.\n",
      "En total representaban 153 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t6_Nexus.csv contenía 4141 datos.\n",
      "En total representaban 142 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t7_Nexus.csv contenía 2159 datos.\n",
      "En total representaban 88 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t8_Nexus.csv contenía 5033 datos.\n",
      "En total representaban 155 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-29062021/Nexus/t9_Nexus.csv contenía 3898 datos.\n",
      "En total representaban 104 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t1_Nexus.csv contenía 3164 datos.\n",
      "En total representaban 108 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t2_Nexus.csv contenía 4773 datos.\n",
      "En total representaban 147 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t3_Nexus.csv contenía 1899 datos.\n",
      "En total representaban 72 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t4_Nexus.csv contenía 3152 datos.\n",
      "En total representaban 112 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t5_Nexus.csv contenía 3804 datos.\n",
      "En total representaban 134 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t6_Nexus.csv contenía 1856 datos.\n",
      "En total representaban 105 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes06-30062021/Nexus/t7_Nexus.csv contenía 3720 datos.\n",
      "En total representaban 141 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t1_Nexus.csv contenía 2425 datos.\n",
      "En total representaban 50 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t2_Nexus.csv contenía 4423 datos.\n",
      "En total representaban 117 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t3_Nexus.csv contenía 5021 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t4_Nexus.csv contenía 3555 datos.\n",
      "En total representaban 113 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-05072021/Nexus/t5_Nexus.csv contenía 4903 datos.\n",
      "En total representaban 124 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t1_Nexus.csv contenía 4952 datos.\n",
      "En total representaban 156 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t2_Nexus.csv contenía 2813 datos.\n",
      "En total representaban 89 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t3_Nexus.csv contenía 2731 datos.\n",
      "En total representaban 106 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t4_Nexus.csv contenía 4180 datos.\n",
      "En total representaban 134 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t5_Nexus.csv contenía 4674 datos.\n",
      "En total representaban 140 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t6_Nexus.csv contenía 4158 datos.\n",
      "En total representaban 136 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t7_Nexus.csv contenía 5134 datos.\n",
      "En total representaban 151 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-06072021/Nexus/t8_Nexus.csv contenía 5201 datos.\n",
      "En total representaban 153 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t1_Nexus.csv contenía 4376 datos.\n",
      "En total representaban 111 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t2_Nexus.csv contenía 4085 datos.\n",
      "En total representaban 114 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t3_Nexus.csv contenía 4299 datos.\n",
      "En total representaban 118 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t4_Nexus.csv contenía 3231 datos.\n",
      "En total representaban 110 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t5_Nexus.csv contenía 4164 datos.\n",
      "En total representaban 127 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t6_Nexus.csv contenía 4261 datos.\n",
      "En total representaban 116 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t7_Nexus.csv contenía 5271 datos.\n",
      "En total representaban 158 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-07072021/Nexus/t8_Nexus.csv contenía 3467 datos.\n",
      "En total representaban 112 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t1_Nexus.csv contenía 2727 datos.\n",
      "En total representaban 94 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t2_Nexus.csv contenía 4232 datos.\n",
      "En total representaban 141 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t3_Nexus.csv contenía 2220 datos.\n",
      "En total representaban 96 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t4_Nexus.csv contenía 3860 datos.\n",
      "En total representaban 141 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t5_Nexus.csv contenía 3955 datos.\n",
      "En total representaban 122 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t6_Nexus.csv contenía 2341 datos.\n",
      "En total representaban 111 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t7_Nexus.csv contenía 4011 datos.\n",
      "En total representaban 122 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-08072021/Nexus/t8_Nexus.csv contenía 2133 datos.\n",
      "En total representaban 87 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t1_Nexus.csv contenía 3760 datos.\n",
      "En total representaban 112 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t2_Nexus.csv contenía 4981 datos.\n",
      "En total representaban 142 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t3_Nexus.csv contenía 4687 datos.\n",
      "En total representaban 128 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t4_Nexus.csv contenía 5805 datos.\n",
      "En total representaban 155 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t5_Nexus.csv contenía 4641 datos.\n",
      "En total representaban 131 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-09072021/Nexus/t6_Nexus.csv contenía 5601 datos.\n",
      "En total representaban 158 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t10_Nexus.csv contenía 5134 datos.\n",
      "En total representaban 152 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t11_Nexus.csv contenía 4504 datos.\n",
      "En total representaban 141 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t1_Nexus.csv contenía 4308 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t2_Nexus.csv contenía 3216 datos.\n",
      "En total representaban 110 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t3_Nexus.csv contenía 3856 datos.\n",
      "En total representaban 118 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t4_Nexus.csv contenía 5453 datos.\n",
      "En total representaban 162 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t5_Nexus.csv contenía 5353 datos.\n",
      "En total representaban 170 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t6_Nexus.csv contenía 4106 datos.\n",
      "En total representaban 128 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t7_Nexus.csv contenía 5599 datos.\n",
      "En total representaban 155 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t8_Nexus.csv contenía 5101 datos.\n",
      "En total representaban 145 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-12072021/Nexus/t9_Nexus.csv contenía 3842 datos.\n",
      "En total representaban 131 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t1_Nexus.csv contenía 4072 datos.\n",
      "En total representaban 143 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t2_Nexus.csv contenía 5616 datos.\n",
      "En total representaban 157 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t3_Nexus.csv contenía 5210 datos.\n",
      "En total representaban 149 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t4_Nexus.csv contenía 7095 datos.\n",
      "En total representaban 199 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t5_Nexus.csv contenía 6957 datos.\n",
      "En total representaban 208 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t6_Nexus.csv contenía 6339 datos.\n",
      "En total representaban 183 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t7_Nexus.csv contenía 7635 datos.\n",
      "En total representaban 229 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-14072021/Nexus/t8_Nexus.csv contenía 6870 datos.\n",
      "En total representaban 174 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t1_Nexus.csv contenía 4194 datos.\n",
      "En total representaban 135 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t2_Nexus.csv contenía 5763 datos.\n",
      "En total representaban 185 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t3_Nexus.csv contenía 6831 datos.\n",
      "En total representaban 206 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t4_Nexus.csv contenía 6975 datos.\n",
      "En total representaban 197 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t5_Nexus.csv contenía 6697 datos.\n",
      "En total representaban 217 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t6_Nexus.csv contenía 5958 datos.\n",
      "En total representaban 177 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-15072021/Nexus/t7_Nexus.csv contenía 5401 datos.\n",
      "En total representaban 143 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t1_Nexus.csv contenía 5175 datos.\n",
      "En total representaban 175 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t2_Nexus.csv contenía 3279 datos.\n",
      "En total representaban 98 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t3_Nexus.csv contenía 3114 datos.\n",
      "En total representaban 126 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t4_Nexus.csv contenía 3772 datos.\n",
      "En total representaban 134 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t5_Nexus.csv contenía 3770 datos.\n",
      "En total representaban 126 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t6_Nexus.csv contenía 4745 datos.\n",
      "En total representaban 171 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-16072021/Nexus/t7_Nexus.csv contenía 3128 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t1_Nexus.csv contenía 2507 datos.\n",
      "En total representaban 116 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t2_Nexus.csv contenía 4397 datos.\n",
      "En total representaban 167 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t3_Nexus.csv contenía 3501 datos.\n",
      "En total representaban 144 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t4_Nexus.csv contenía 4254 datos.\n",
      "En total representaban 164 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t5_Nexus.csv contenía 3023 datos.\n",
      "En total representaban 136 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t6_Nexus.csv contenía 3368 datos.\n",
      "En total representaban 144 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t7_Nexus.csv contenía 3506 datos.\n",
      "En total representaban 132 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-19072021/Nexus/t8_Nexus.csv contenía 2950 datos.\n",
      "En total representaban 121 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t1_Nexus.csv contenía 4696 datos.\n",
      "En total representaban 161 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t2_Nexus.csv contenía 4607 datos.\n",
      "En total representaban 138 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t3_Nexus.csv contenía 3413 datos.\n",
      "En total representaban 108 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t4_Nexus.csv contenía 3691 datos.\n",
      "En total representaban 119 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t5_Nexus.csv contenía 4445 datos.\n",
      "En total representaban 162 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t6_Nexus.csv contenía 4920 datos.\n",
      "En total representaban 168 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t7_Nexus.csv contenía 5688 datos.\n",
      "En total representaban 203 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-20072021/Nexus/t8_Nexus.csv contenía 3308 datos.\n",
      "En total representaban 119 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t1_Nexus.csv contenía 2934 datos.\n",
      "En total representaban 118 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t2_Nexus.csv contenía 3140 datos.\n",
      "En total representaban 128 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t3_Nexus.csv contenía 2539 datos.\n",
      "En total representaban 117 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t4_Nexus.csv contenía 2973 datos.\n",
      "En total representaban 129 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t5_Nexus.csv contenía 3389 datos.\n",
      "En total representaban 146 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t6_Nexus.csv contenía 2979 datos.\n",
      "En total representaban 112 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t7_Nexus.csv contenía 2159 datos.\n",
      "En total representaban 111 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-22072021/Nexus/t8_Nexus.csv contenía 2617 datos.\n",
      "En total representaban 106 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t1_Nexus.csv contenía 4344 datos.\n",
      "En total representaban 148 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t2_Nexus.csv contenía 2390 datos.\n",
      "En total representaban 107 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t3_Nexus.csv contenía 3523 datos.\n",
      "En total representaban 140 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t4_Nexus.csv contenía 3726 datos.\n",
      "En total representaban 149 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t5_Nexus.csv contenía 2798 datos.\n",
      "En total representaban 128 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t6_Nexus.csv contenía 3046 datos.\n",
      "En total representaban 135 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-23072021/Nexus/t7_Nexus.csv contenía 2604 datos.\n",
      "En total representaban 105 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t1_Nexus.csv contenía 3793 datos.\n",
      "En total representaban 121 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t2_Nexus.csv contenía 3977 datos.\n",
      "En total representaban 120 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t3_Nexus.csv contenía 2527 datos.\n",
      "En total representaban 87 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t4_Nexus.csv contenía 4371 datos.\n",
      "En total representaban 120 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-27072021/Nexus/t5_Nexus.csv contenía 2642 datos.\n",
      "En total representaban 96 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t1_Nexus.csv contenía 4425 datos.\n",
      "En total representaban 129 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t2_Nexus.csv contenía 3249 datos.\n",
      "En total representaban 114 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t3_Nexus.csv contenía 4958 datos.\n",
      "En total representaban 167 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t4_Nexus.csv contenía 4703 datos.\n",
      "En total representaban 143 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t5_Nexus.csv contenía 4866 datos.\n",
      "En total representaban 157 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-28072021/Nexus/t6_Nexus.csv contenía 3181 datos.\n",
      "En total representaban 115 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t1_Nexus.csv contenía 3637 datos.\n",
      "En total representaban 136 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t2_Nexus.csv contenía 3195 datos.\n",
      "En total representaban 143 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t3_Nexus.csv contenía 2976 datos.\n",
      "En total representaban 116 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t4_Nexus.csv contenía 5154 datos.\n",
      "En total representaban 191 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t5_Nexus.csv contenía 3229 datos.\n",
      "En total representaban 121 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t6_Nexus.csv contenía 3242 datos.\n",
      "En total representaban 141 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t7_Nexus.csv contenía 4754 datos.\n",
      "En total representaban 179 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes07-29072021/Nexus/t8_Nexus.csv contenía 3468 datos.\n",
      "En total representaban 129 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t1_Nexus.csv contenía 2125 datos.\n",
      "En total representaban 101 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t2_Nexus.csv contenía 3546 datos.\n",
      "En total representaban 138 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t3_Nexus.csv contenía 3059 datos.\n",
      "En total representaban 117 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t4_Nexus.csv contenía 4328 datos.\n",
      "En total representaban 154 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t5_Nexus.csv contenía 5031 datos.\n",
      "En total representaban 205 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t6_Nexus.csv contenía 2221 datos.\n",
      "En total representaban 100 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t7_Nexus.csv contenía 3101 datos.\n",
      "En total representaban 113 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-04082021/Nexus/t8_Nexus.csv contenía 2655 datos.\n",
      "En total representaban 108 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t1_Nexus.csv contenía 3733 datos.\n",
      "En total representaban 142 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t2_Nexus.csv contenía 4427 datos.\n",
      "En total representaban 160 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t3_Nexus.csv contenía 2291 datos.\n",
      "En total representaban 98 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t4_Nexus.csv contenía 3858 datos.\n",
      "En total representaban 140 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t5_Nexus.csv contenía 5008 datos.\n",
      "En total representaban 183 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t6_Nexus.csv contenía 2889 datos.\n",
      "En total representaban 115 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t7_Nexus.csv contenía 4485 datos.\n",
      "En total representaban 151 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-05082021/Nexus/t8_Nexus.csv contenía 3344 datos.\n",
      "En total representaban 110 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t1_Nexus.csv contenía 2625 datos.\n",
      "En total representaban 106 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t2_Nexus.csv contenía 4058 datos.\n",
      "En total representaban 150 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t3_Nexus.csv contenía 2790 datos.\n",
      "En total representaban 120 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t4_Nexus.csv contenía 2905 datos.\n",
      "En total representaban 117 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t5_Nexus.csv contenía 4206 datos.\n",
      "En total representaban 146 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t6_Nexus.csv contenía 5699 datos.\n",
      "En total representaban 195 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t7_Nexus.csv contenía 4691 datos.\n",
      "En total representaban 188 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-10082021/Nexus/t8_Nexus.csv contenía 5169 datos.\n",
      "En total representaban 172 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t1_Nexus.csv contenía 2249 datos.\n",
      "En total representaban 102 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t2_Nexus.csv contenía 3356 datos.\n",
      "En total representaban 124 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t3_Nexus.csv contenía 3728 datos.\n",
      "En total representaban 147 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t4_Nexus.csv contenía 2915 datos.\n",
      "En total representaban 118 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t5_Nexus.csv contenía 3490 datos.\n",
      "En total representaban 133 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t6_Nexus.csv contenía 2784 datos.\n",
      "En total representaban 103 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t7_Nexus.csv contenía 3437 datos.\n",
      "En total representaban 124 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Train/Mes08-12082021/Nexus/t8_Nexus.csv contenía 2203 datos.\n",
      "En total representaban 97 secuencias.\n",
      "Por lo que el total de datos a procesar tiene que ser de 661805\n",
      "Al realizar el 'aplanamiento' nos quedamos con un total de 661805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha creado la variable matriz_Train\n",
      "\u001b[1mSet de testeo\u001b[0m\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t8_Nexus.csv\n",
      "En total se han descargado 8 ficheros, los cuales se colocarán siguiendo el orden que se muestra a continuación:\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t1_Nexus.csv contenía 6131 datos.\n",
      "En total representaban 145 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t2_Nexus.csv contenía 6551 datos.\n",
      "En total representaban 165 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t3_Nexus.csv contenía 6110 datos.\n",
      "En total representaban 151 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t4_Nexus.csv contenía 5021 datos.\n",
      "En total representaban 140 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t5_Nexus.csv contenía 4081 datos.\n",
      "En total representaban 122 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t6_Nexus.csv contenía 5352 datos.\n",
      "En total representaban 140 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t7_Nexus.csv contenía 4433 datos.\n",
      "En total representaban 135 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Test/Mes09-10092021/Nexus/t8_Nexus.csv contenía 5854 datos.\n",
      "En total representaban 142 secuencias.\n",
      "Por lo que el total de datos a procesar tiene que ser de 43533\n",
      "Al realizar el 'aplanamiento' nos quedamos con un total de 43533\n",
      "Se ha creado la variable matriz_Test\n",
      "\u001b[1mSet de validación\u001b[0m\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t8_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t1_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t2_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t3_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t4_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t5_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t6_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t7_Nexus.csv\n",
      "/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t8_Nexus.csv\n",
      "En total se han descargado 37 ficheros, los cuales se colocarán siguiendo el orden que se muestra a continuación:\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t1_Nexus.csv contenía 4812 datos.\n",
      "En total representaban 157 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t2_Nexus.csv contenía 4612 datos.\n",
      "En total representaban 158 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t3_Nexus.csv contenía 3755 datos.\n",
      "En total representaban 132 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t4_Nexus.csv contenía 5180 datos.\n",
      "En total representaban 175 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t5_Nexus.csv contenía 2526 datos.\n",
      "En total representaban 113 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t6_Nexus.csv contenía 4426 datos.\n",
      "En total representaban 161 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t7_Nexus.csv contenía 4829 datos.\n",
      "En total representaban 177 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-16082021/Nexus/t8_Nexus.csv contenía 4406 datos.\n",
      "En total representaban 168 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t1_Nexus.csv contenía 2525 datos.\n",
      "En total representaban 105 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t2_Nexus.csv contenía 3872 datos.\n",
      "En total representaban 144 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t3_Nexus.csv contenía 4001 datos.\n",
      "En total representaban 146 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t4_Nexus.csv contenía 4203 datos.\n",
      "En total representaban 164 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t5_Nexus.csv contenía 3715 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-18082021/Nexus/t6_Nexus.csv contenía 3910 datos.\n",
      "En total representaban 149 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t1_Nexus.csv contenía 4215 datos.\n",
      "En total representaban 137 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t2_Nexus.csv contenía 3061 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t3_Nexus.csv contenía 5103 datos.\n",
      "En total representaban 165 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t4_Nexus.csv contenía 3513 datos.\n",
      "En total representaban 100 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t5_Nexus.csv contenía 3258 datos.\n",
      "En total representaban 121 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t6_Nexus.csv contenía 3693 datos.\n",
      "En total representaban 123 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-23082021/Nexus/t7_Nexus.csv contenía 2489 datos.\n",
      "En total representaban 91 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t1_Nexus.csv contenía 2424 datos.\n",
      "En total representaban 103 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t2_Nexus.csv contenía 4328 datos.\n",
      "En total representaban 161 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t3_Nexus.csv contenía 4759 datos.\n",
      "En total representaban 162 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t4_Nexus.csv contenía 3745 datos.\n",
      "En total representaban 131 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t5_Nexus.csv contenía 2753 datos.\n",
      "En total representaban 119 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t6_Nexus.csv contenía 3304 datos.\n",
      "En total representaban 132 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t7_Nexus.csv contenía 3012 datos.\n",
      "En total representaban 121 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-24082021/Nexus/t8_Nexus.csv contenía 3350 datos.\n",
      "En total representaban 97 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t1_Nexus.csv contenía 2700 datos.\n",
      "En total representaban 89 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t2_Nexus.csv contenía 5528 datos.\n",
      "En total representaban 161 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t3_Nexus.csv contenía 6615 datos.\n",
      "En total representaban 203 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t4_Nexus.csv contenía 5229 datos.\n",
      "En total representaban 162 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t5_Nexus.csv contenía 8133 datos.\n",
      "En total representaban 233 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t6_Nexus.csv contenía 5488 datos.\n",
      "En total representaban 169 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t7_Nexus.csv contenía 6731 datos.\n",
      "En total representaban 180 secuencias.\n",
      "El archivo '/home/laura/Adrian/Procesaor_dataset/Database/Raw_data/Val/Mes08-26082021/Nexus/t8_Nexus.csv contenía 4896 datos.\n",
      "En total representaban 129 secuencias.\n",
      "Por lo que el total de datos a procesar tiene que ser de 155099\n",
      "Al realizar el 'aplanamiento' nos quedamos con un total de 155099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha creado la variable matriz_Val\n",
      "\n",
      "⏳0:0:5\n"
     ]
    }
   ],
   "source": [
    "#Creamos las listas de entrenamiento, testeo y validación\n",
    "if(\"direcciones_Train\" in globals()):\n",
    "    print('\\033[1m'+'Set de entrenamiento'+'\\033[0m')\n",
    "    str_info = str_info + \"\\n\\u25BA Extracción de datos del set de entrenamiento \\u25C4\\n\"\n",
    "    matriz_Train, secuencias_Train, listado_base_Train, orden_Train = Saca_matrices(direcciones_Train)\n",
    "    print(\"Se ha creado la variable matriz_Train\")\n",
    "    \n",
    "else:\n",
    "    if(\"matriz_Train\" in globals()): del matriz_Train\n",
    "    if(\"secuencias_Train\" in globals()): del secuencias_Train\n",
    "    if(\"listado_base_Train\" in globals()): del listado_base_Train\n",
    "    if(\"orden_Train\" in globals()):del orden_Train\n",
    "\n",
    "if(\"direcciones_Test\" in globals()):\n",
    "    print('\\033[1m'+'Set de testeo'+'\\033[0m')\n",
    "    str_info = str_info + \"\\n\\u25BA Extracción de datos del set de testeo \\u25C4\\n\"\n",
    "    matriz_Test, secuencias_Test, listado_base_Test, orden_Test = Saca_matrices(direcciones_Test)\n",
    "    print(\"Se ha creado la variable matriz_Test\")\n",
    "else:\n",
    "    if(\"matriz_Test\" in globals()): del matriz_Test\n",
    "    if(\"secuencias_Test\" in globals()): del secuencias_Test\n",
    "    if(\"listado_base_Test\" in globals()): del listado_base_Test\n",
    "    if(\"orden_Test\" in globals()):del orden_Test\n",
    "        \n",
    "if(\"direcciones_Val\" in globals()):\n",
    "    print('\\033[1m'+'Set de validación'+'\\033[0m')\n",
    "    str_info = str_info + \"\\n\\u25BA Extracción de datos del set de validación \\u25C4\\n\"\n",
    "    matriz_Val, secuencias_Val, listado_base_Val, orden_Val = Saca_matrices(direcciones_Val)\n",
    "    print(\"Se ha creado la variable matriz_Val\")\n",
    "else:\n",
    "    if(\"matriz_Val\" in globals()): del matriz_Val\n",
    "    if(\"secuencias_Val\" in globals()): del secuencias_Val\n",
    "    if(\"listado_base_Val\" in globals()): del listado_base_Val\n",
    "    if(\"orden_Val\" in globals()):del orden_Val\n",
    "    \n",
    "#Recuento del tiempo\n",
    "tiempo_datos = time.time()\n",
    "tiempo_total = tiempo_datos-tiempo_inicio\n",
    "\n",
    "segundos=tiempo_total\n",
    " \n",
    "horas=int(segundos/3600)\n",
    "segundos-=horas*3600\n",
    "minutos=int(segundos/60)\n",
    "segundos-=int(minutos*60)\n",
    "segundos =int(segundos)\n",
    "\n",
    "print(\"\\n\\u23F3%s:%s:%s\" % (horas,minutos,segundos))\n",
    "str_info = str_info + \"\\n\\t\\u23F3 En total el proceso de extracción de datos ha tardado \" + str(horas) +\":\"+str(minutos)+\":\"+str(segundos)+\".\\n\" \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141464da",
   "metadata": {},
   "source": [
    "## Procesado de los datos\n",
    "\n",
    "Esta parte del código se encargará de procesar las matrices calculadas anteriormente para darlas el formato adecuado antes de exportarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207cf51",
   "metadata": {},
   "source": [
    "### Obtención de las listas de AP's\n",
    "\n",
    "Lo primero será comprobar la existencia de alguna lista a la que aferrarse. En el caso de que exista los datos se acomodarán a ella, de lo contrario habrá distintas maneras de proceder.\n",
    "\n",
    "Para el caso del entrenamiento, si no hay una lista preestablicida (que es lo esperable) habrá que localizar los diferentes puntos de acceso que aparecen en todos los datos dentro de un conjunto, los cuales pueden no conincidir con los de otros conjuntos (por ejemplo los APs vistos en el entrenamiento pueden ser distintos de los vistos en el testeo).\n",
    "Los APs vistos en el entrenamiento marcaran el orden de la matriz, mientras que los de testeo y validación se tendran que ajustar a dicho orden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1194a203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha encontrado la lista: listado_base_Train\n",
      "El elemento estaba vacio, así que pasamos a borrarlo\n",
      "Se ha encontrado la lista: listado_base_Test\n",
      "El elemento estaba vacio, así que pasamos a borrarlo\n",
      "Se ha encontrado la lista: listado_base_Val\n",
      "El elemento estaba vacio, así que pasamos a borrarlo\n",
      "Como ha seleccionado la opción para no ordenar la lista, se esta procesando usando un bucle for, por lo que este paso puede tomar un poco de tiempo.\n",
      "Entre los datos de entrenamiento se han encontrado un total de 1246 direcciones MAC diferentes. \n",
      "Aquí te muestro las 10 primeras:\n",
      "['ba:6c:01:22:d0:b0' '1c:64:99:5c:59:a4' 'f0:92:1c:ab:7b:6c'\n",
      " 'c0:fd:84:d6:e0:33' '78:dd:12:26:9f:cc' '64:cc:22:f7:b6:44'\n",
      " '94:91:7f:0d:58:c8' 'c6:ed:dc:80:20:06' 'cc:ed:dc:80:20:06'\n",
      " '34:57:60:9f:7c:f3']\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos si hay alguna lista y limpiamos las que haya (si tienen indices Latitud o Longitud los eliminamos)\n",
    "lista_listas=[\n",
    "    \"listado_base_Train\",\n",
    "    \"listado_base_Test\",\n",
    "    \"listado_base_Val\"\n",
    "]\n",
    "\n",
    "lista_filtros=[\n",
    "    \"Latitud\",\n",
    "    \"Longitud\"\n",
    "]\n",
    "\n",
    "str_info = str_info + \"\\n\\u25BA Obtención de las listas \\u25C4\\n\"\n",
    "\n",
    "for element in lista_listas:\n",
    "    if((element in globals()) & (element is not None)):\n",
    "        print(\"Se ha encontrado la lista: \"+str(element))\n",
    "        if (globals()['%s' % element] is None):\n",
    "            print(\"El elemento estaba vacio, así que pasamos a borrarlo\")\n",
    "            del (globals()['%s' % element])\n",
    "        else:\n",
    "            print(\"La lista está formada por \" +str(len(globals()['%s' % element]))+ \" APs. Mostramos las primeras 10 filas de la lista:\\n\" +str(globals()['%s' % element][0:10]))\n",
    "            str_info = str_info + \"Se ha encontrado la lista: \"+str(element) + \" formada por \" +str(len(globals()['%s' % element]))+ \" APs. Mostramos las primeras 10 filas de la lista:\\n\" +str(globals()['%s' % element][0:10]) +\"\\n\"\n",
    "            \n",
    "            #Revisamos que no haya columnas \"Latitud\" o \"Longitud\"\n",
    "            for filtro in lista_filtros:     \n",
    "                if(filtro in globals()[\"%s\"%element]):\n",
    "                    #print(\"Encontrada columna \"+filtro+\" en \" + element + \". Procedemos a borrarla.\")\n",
    "                    posicion = np.where(globals()[\"%s\"%element]==filtro)[0][0]\n",
    "                    #print(posicion)\n",
    "                    globals()[\"%s\"%element]=np.delete(globals()[\"%s\"%element], posicion)\n",
    "                    print(\"En la lista original se encontraron columnas que sobran ('Latitud o Longitud'), tras borrarlas nos quedamos con una lista base de tamaño \"+ str(len(globals()[\"%s\"%element])))\n",
    "                    str_info = str_info + \"En la lista original se encontraron columnas que sobran ('Latitud o Longitud'), tras borrarlas nos quedamos con una lista base de tamaño \"+ str(len(globals()[\"%s\"%element]))\n",
    "                    \n",
    "if(\"matriz_Train\" in globals()):\n",
    "    if(\"listado_base_Train\" not in globals()):\n",
    "        #Filtramos en función de las direcciones MAC, las cuales se presentan en la 3 columna\n",
    "        matriz_Aps = np.zeros(matriz_Train.shape[0])\n",
    "        matriz_Aps = matriz_Train[:,2]\n",
    "\n",
    "        #Nos quedamos solo con uno de cada para crear la lista\n",
    "        if(ordenar_listas==False):\n",
    "            #Si no queremos ordenar la lista mostramos las direcciones MAC conforme vayan apareciendo\n",
    "            print(\"Como ha seleccionado la opción para no ordenar la lista, se esta procesando usando un bucle for, por lo que este paso puede tomar un poco de tiempo.\")\n",
    "            Aps_unicos = []            \n",
    "            for element in matriz_Aps:\n",
    "                if element not in Aps_unicos: Aps_unicos.append(element)\n",
    "            Aps_unicos=np.array(Aps_unicos)\n",
    "            str_info = str_info + \"El listado con las direcciones MAC se ha procesado manualmente ya que ha seleccionado la opción de no ordenarlo.\"\n",
    "            \n",
    "        else:\n",
    "            Aps_unicos = np.unique(matriz_Aps)    \n",
    "            str_info = str_info + \"El listado ha sido ordenado de forma ascendente en función de las direcciones MAC.\"\n",
    "        \n",
    "        print(\"Entre los datos de entrenamiento se han encontrado un total de \"+ str(len(Aps_unicos))+\" direcciones MAC diferentes. \\nAquí te muestro las 10 primeras:\\n\"+ str(Aps_unicos[0:10]) )\n",
    "        listado_base_Train = Aps_unicos\n",
    "        \n",
    "        str_info = str_info + \"Hemos procesado los datos de entrenamiento. En total hemos detectado \" +str(len(Aps_unicos))+\" direcciones MAC únicas.\" + \"Aquí te muestro las 10 primeras:\\n\"+ str(Aps_unicos[0:10]) + \"\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13b0d53",
   "metadata": {},
   "source": [
    "### Funciónes para ordenar los datos\n",
    "Las siguientes funciones sirven para organizar los datos y crear las matrices finales con las que trabajaremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767f63a",
   "metadata": {},
   "source": [
    "En el caso de la matriz de entrenamiento esta recibe como parámetros:\n",
    "* Identificadores: Una array con las direcciones MAC únicas filtradas anteriormente\n",
    "* Matriz_scan: La matriz en la que aparecen los datos leidos de los csv creada anteriormente\n",
    "* Etiquetas_juntas (opcional): En caso de que este parámetro sea verdadero las etiquetas se incluirán en la matriz final, de lo contrario se crearán dos matrices separadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f880988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Organizador_entrenamiento(matriz_scan, secuencias, identificadores, etiquetas_juntas=False, add_time=False):\n",
    "    #En la primera columna de la matriz se almacena el número de escaneo, así que para saber cuantos escaneos hay leemos el valor de la primera columna de la última fila\n",
    "    numero_scaneos=sum(secuencias)+len(secuencias) #Como empiezan en 0 sumamos 1 por cada secuencia\n",
    "    print(\"Localizados \"+str(numero_scaneos)+\" escaneos distintos\")\n",
    "    globals()[\"str_info\"]=globals()[\"str_info\"] + \"Localizados \"+str(numero_scaneos)+\" escaneos distintos.\\n\"\n",
    "    \n",
    "    #Definimos el tamaño de la matriz con los APs\n",
    "    matriz_salida=np.ones((numero_scaneos,len(identificadores)))*(inv_value)\n",
    "    #Definimos el tamaño de la matriz de etiquetas\n",
    "    matriz_etiquetas=np.zeros((numero_scaneos,2))\n",
    "    #Definimos el tamaño de la matriz de tiempos. Como son strings hay que definir el tamaño de cada item\n",
    "    matriz_time=np.chararray((numero_scaneos,1),itemsize = 27, unicode = True)\n",
    "    \n",
    "    set_datos = 0\n",
    "    offset = 0\n",
    "    muestra_anterior = 0\n",
    "    \n",
    "    #Colocamos los datos de forma ordenada según aparezcan en la lista de identificadores\n",
    "    for ciclo, element in enumerate(matriz_scan):\n",
    "        #Nos aseguramos que la dirección MAC este en la lista, si no algo ha fallado\n",
    "        assert element[2] in identificadores.tolist(), \"La dirección MAC \"+str(element[2])+\" del elemento \"+str(ciclo)+\" no se había listado.\"\n",
    "        \n",
    "        if((int(element[0])!=int(muestra_anterior)) & (int(muestra_anterior) ==secuencias[set_datos])):\n",
    "            offset = offset + secuencias[set_datos] +1\n",
    "            set_datos=set_datos+1\n",
    "            \n",
    "        \n",
    "        fila = offset + int(element[0])\n",
    "        #print(fila, offset, int(element[0]),secuencias[set_datos])\n",
    "        columna = np.where(identificadores == element[2])\n",
    "        \n",
    "        matriz_salida[fila,int(columna[0])] = element[3]\n",
    "        matriz_etiquetas[fila] = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', str(element[5]))]\n",
    "\n",
    "        #Guardamos las marcas de tiempo\n",
    "        matriz_time[fila] = element[4]\n",
    "        #print(matriz_time[fila], type(element[4]), len(element[4]))\n",
    "        \n",
    "        muestra_anterior = element[0]\n",
    "        #print(\"Fila: \"+str(fila)+\" columna: \"+str(columna))\n",
    "    \n",
    "    listado = identificadores\n",
    "        \n",
    "    #Si está indicado que se añadan las etiquetas\n",
    "    if(etiquetas_juntas == True):\n",
    "        matriz_salida = np.concatenate((matriz_salida, matriz_etiquetas), axis=1)\n",
    "        matriz_etiquetas = None\n",
    "        listado = np.concatenate((listado, [\"Latitud\",\"Longitud\"]), axis=0)\n",
    "        \n",
    "    #Si está indicado que se añadan las marcas de tiempo\n",
    "    if(add_time == True):\n",
    "        print(\"He entrado en add time.\")\n",
    "        matriz_salida = np.concatenate((matriz_salida, matriz_time), axis=1)\n",
    "        matriz_time = None\n",
    "        listado = np.concatenate((listado, [\"Time stamp\"]), axis=0)\n",
    "    \n",
    "    return (matriz_salida, matriz_etiquetas, matriz_time, listado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3cab0",
   "metadata": {},
   "source": [
    "En el caso del testeo y validación existen varias posibilidades:\n",
    "* En caso de que se le introduzca una lista de APs (por ejemplo la del entrenamiento) los datos se acomodarán a la misma, dejando a elección del usuario si borrar los APs que no aparezcan en la lista o si añadirlos al final.\n",
    "* Si no se introduce una lista base se procesará la misma y e acomodarán los datos.\n",
    "En lo que respecta a las etiquetas lo gestionamos al igual que en el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc20bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Organizador_general(matriz_scan, secuencias, identificadores=None, borrar_nuevos=False, etiquetas_juntas=False, add_time=False):\n",
    "    #En la primera columna de la matriz se almacena el número de escaneo, así que para saber cuantos escaneos hay leemos el valor de la primera columna de la última fila\n",
    "    numero_scaneos=sum(secuencias)+len(secuencias) #Como empiezan en 0 sumamos 1 por cada secuencia\n",
    "    print(\"Localizados \"+str(numero_scaneos)+\" escaneos distintos\")\n",
    "    globals()[\"str_info\"]=globals()[\"str_info\"] + \"Localizados \"+str(numero_scaneos)+\" escaneos distintos.\\n\"\n",
    "    \n",
    "    cuenta=0\n",
    "    set_datos = 0\n",
    "    offset = 0\n",
    "    muestra_anterior = 0\n",
    "    \n",
    "    #Si se ha introducido una lista de etiquetas debemos seguirla\n",
    "    if identificadores is not None:\n",
    "        lista_Aps = identificadores\n",
    "        print(\"La lista con los APs original era de tamaño \"+str(len(lista_Aps)))\n",
    "        \n",
    "        if(borrar_nuevos == False):\n",
    "            #Comprobamos si la direccion MAC pertenece al listado, y de no ser así la añadimos al final\n",
    "            for element in matriz_scan:\n",
    "                if(element[2] not in lista_Aps.tolist()):\n",
    "                    lista_Aps = np.append(lista_Aps, element[2])\n",
    "                    cuenta=cuenta+1\n",
    "                    #print(\"La señal: \"+str(element)+\" no pertenece al listado\")\n",
    "            print(\"Tras revisar los datos de entrada se han encontrado \"+str(cuenta)+\" APs nuevos, por lo que finalmente se han listado \"+str(len(lista_Aps))+\" Aps.\")\n",
    "            globals()[\"str_info\"]=globals()[\"str_info\"] + \"[Importante]: La lista con los APs original era de tamaño \"+str(len(identificadores))+ \". Tras revisar los datos de entrada se han encontrado \"+str(cuenta)+\" APs nuevos, por lo que finalmente se han listado \"+str(len(lista_Aps))+\" Aps.\\n\"\n",
    "\n",
    "        else:\n",
    "            print(\"[Importante]: Seleccionada la opción para omitir los APs que no aparezcan en la lista original (ya sea la introducida manualmente o la generada en el entrenamiento)\")\n",
    "            globals()[\"str_info\"]=globals()[\"str_info\"] + \"[Importante]: Seleccionada la opción para omitir los APs que no aparezcan en la lista original (ya sea la introducida manualmente o la generada en el entrenamiento)\"\n",
    "            \n",
    "    \n",
    "    #Si no se introduce una lista para organizar los AP creamos una propia\n",
    "    else:\n",
    "        #Creamos la lista de los diferentes APs\n",
    "        Aps_unicos = np.zeros(matriz_scan.shape[0])\n",
    "        Aps_unicos = matriz_scan[:,2]\n",
    "        \n",
    "        if(ordenar_listas==False):\n",
    "            #Si no queremos ordenar la lista mostramos las direcciones MAC conforme vayan apareciendo\n",
    "            print(\"Como ha seleccionado la opción para no ordenar la lista, se esta procesando usando un bucle for, por lo que este paso puede tomar un poco de tiempo.\")\n",
    "            lista_Aps = []            \n",
    "            for element in Aps_unicos:\n",
    "                if element not in lista_Aps: lista_Aps.append(element)\n",
    "            lista_Aps=np.array(lista_Aps)\n",
    "            \n",
    "        else:\n",
    "            lista_Aps = np.unique(Aps_unicos)\n",
    "        \n",
    "        print(\"No se ha introducido ninguna lista, por lo que se procede a organizar los APs conforme aparecen en los csv.\\nEn total se han encontrado \"+ str(len(Aps_unicos))+\" direcciones MAC diferentes. Aquí te muestro las 10 primeras:\\n\"+ str(Aps_unicos[0:10]) )\n",
    "        globals()[\"str_info\"]=globals()[\"str_info\"] + \"No se ha introducido ninguna lista, por lo que se procede a organizar los APs conforme aparecen en los csv.\\nEn total se han encontrado \"+ str(len(Aps_unicos))+\" direcciones MAC diferentes. Aquí te muestro las 10 primeras:\\n\"+ str(Aps_unicos[0:10])+\"\\n\"\n",
    "        \n",
    "    #Definimos el tamaño de la matriz con los APs\n",
    "    matriz_salida=np.ones((numero_scaneos,len(lista_Aps)))*(inv_value)\n",
    "    #Definimos el tamaño de la matriz de etiquetas\n",
    "    matriz_etiquetas=np.zeros((numero_scaneos,2))\n",
    "    #Definimos el tamaño de la matriz de tiempos\n",
    "    matriz_time=np.chararray((numero_scaneos,1),itemsize = 27, unicode = True)\n",
    "        \n",
    "    #Colocamos los datos de forma ordenada según aparezcan en la lista de identificadores\n",
    "    for ciclo, element in enumerate(matriz_scan):\n",
    "        \n",
    "        #Si no borras los APs fuera de la lista los pones al final según vayan apareciendo\n",
    "        if(borrar_nuevos == False):\n",
    "            #Nos aseguramos que la dirección MAC este en la lista, si no algo ha fallado\n",
    "            assert element[2] in lista_Aps.tolist(), \"La dirección MAC \"+str(element[2])+\" del elemento \"+str(ciclo)+\" no se había listado.\"\n",
    "\n",
    "            if((int(element[0])!=int(muestra_anterior)) & (int(muestra_anterior) ==secuencias[set_datos])):\n",
    "                offset = offset + secuencias[set_datos] +1\n",
    "                set_datos=set_datos+1\n",
    "\n",
    "            fila = offset + int(element[0])\n",
    "            #print(fila, offset, int(element[0]),secuencias[set_datos])\n",
    "            columna = np.where(lista_Aps == element[2])\n",
    "            #print(columna[0], element[2])\n",
    "            matriz_salida[fila,int(columna[0])] = element[3]\n",
    "            \n",
    "            #Si hay etiquetas\n",
    "            if(len(element) >= 5):\n",
    "                if(element[5][2]==\".\"):\n",
    "                    matriz_etiquetas[fila] = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', str(element[5]))]\n",
    "                    hay_etiquetas = True\n",
    "                   \n",
    "        \n",
    "        #Si quieres borrar los datos cuando aparezca un AP que no está en la lista no lo añades \n",
    "        else:\n",
    "            if((int(element[0])!=int(muestra_anterior)) & (int(muestra_anterior) ==secuencias[set_datos])):\n",
    "                offset = offset + secuencias[set_datos] +1\n",
    "                set_datos=set_datos+1\n",
    "\n",
    "            fila = offset + int(element[0])\n",
    "            #print(fila, offset, int(element[0]),secuencias[set_datos])\n",
    "            columna = np.where(lista_Aps == element[2])\n",
    "            #Si no ha encontrado el AP en la lista no lo añadimos\n",
    "            if (len(columna[0]) != 0):\n",
    "                #print(columna[0], element[2])\n",
    "                matriz_salida[fila,int(columna[0])] = element[3]\n",
    "        \n",
    "            #Si hay etiquetas\n",
    "            if(len(element) >= 5):\n",
    "                if(element[5][2]==\".\"):\n",
    "                    matriz_etiquetas[fila] = [float(s) for s in re.findall(r'-?\\d+\\.?\\d*', str(element[5]))]\n",
    "                    hay_etiquetas = True\n",
    "                    \n",
    "        muestra_anterior = element[0]\n",
    "        \n",
    "        #Guardamos las marcas de tiempo\n",
    "        matriz_time[int(element[0])] = element[4]\n",
    "            \n",
    "        \n",
    "\n",
    "    #Devolvemos el listado\n",
    "    listado = lista_Aps\n",
    "    \n",
    "    #Si está indicado que se añadan las etiquetas\n",
    "    if(etiquetas_juntas == True & (\"hay_etiquetas\" in locals())):\n",
    "        matriz_salida = np.concatenate((matriz_salida, matriz_etiquetas), axis=1)\n",
    "        matriz_etiquetas = None\n",
    "        listado = np.concatenate((listado, [\"Latitud\",\"Longitud\"]), axis=0)\n",
    "    \n",
    "    #Si está indicado que se añadan las marcas de tiempo\n",
    "    if(add_time == True):\n",
    "        matriz_salida = np.concatenate((matriz_salida, matriz_time), axis=1)\n",
    "        matriz_time = None\n",
    "        listado = np.concatenate((listado, [\"Time stamp\"]), axis=0)\n",
    "    \n",
    "    return (matriz_salida, matriz_etiquetas, matriz_time, listado)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32d261",
   "metadata": {},
   "source": [
    "### Obtención de las matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db1e3e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTrain\u001b[0m\n",
      "Localizados 22610 escaneos distintos\n",
      "He entrado en add time.\n",
      "Resultado de tamaño 22610x1249.\n",
      " Aquí un ejemplo de las primeras 10 filas y columnas:\n",
      "[['-31.0' '-85.0' '-86.0' '-87.0' '-81.0' '-87.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-31.0' '-85.0' '-79.0' '-87.0' '-80.0' '-87.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-31.0' '-85.0' '-74.0' '-83.0' '-80.0' '-87.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-35.0' '-89.0' '-74.0' '-84.0' '-80.0' '-87.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-35.0' '-89.0' '-78.0' '-84.0' '-80.0' '-87.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-35.0' '-89.0' '-71.0' '-88.0' '-80.0' '-81.0' '-87.0' '-91.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-35.0' '-90.0' '-73.0' '-88.0' '-80.0' '-81.0' '-87.0' '-91.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-35.0' '-90.0' '-65.0' '-88.0' '-100.0' '-85.0' '-87.0' '-91.0'\n",
      "  '-79.0' '-87.0']\n",
      " ['-35.0' '-90.0' '-73.0' '-88.0' '-100.0' '-85.0' '-87.0' '-91.0'\n",
      "  '-79.0' '-80.0']\n",
      " ['-35.0' '-90.0' '-77.0' '-88.0' '-100.0' '-85.0' '-87.0' '-91.0'\n",
      "  '-79.0' '-80.0']]\n",
      "\u001b[1mTest\u001b[0m\n",
      "Matriz obtenida a partir de los datos de entrenamiento. Los AP's específicos de esta parte se encuentran al final\n",
      "Localizados 1148 escaneos distintos\n",
      "La lista con los APs original era de tamaño 1246\n",
      "[Importante]: Seleccionada la opción para omitir los APs que no aparezcan en la lista original (ya sea la introducida manualmente o la generada en el entrenamiento)\n",
      "Resultado de tamaño 1148x1249.\n",
      " Aquí un ejemplo de las primeras 10 filas y columnas:\n",
      "[['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']]\n",
      "\u001b[1mVal\u001b[0m\n",
      "Matriz obtenida a partir de los datos de entrenamiento. Los AP's específicos de esta parte se encuentran al final\n",
      "Localizados 5321 escaneos distintos\n",
      "La lista con los APs original era de tamaño 1246\n",
      "[Importante]: Seleccionada la opción para omitir los APs que no aparezcan en la lista original (ya sea la introducida manualmente o la generada en el entrenamiento)\n",
      "Resultado de tamaño 5321x1249.\n",
      " Aquí un ejemplo de las primeras 10 filas y columnas:\n",
      "[['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']\n",
      " ['-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0' '-100.0'\n",
      "  '-100.0' '-100.0']]\n",
      "\n",
      "⏳0:1:59\n"
     ]
    }
   ],
   "source": [
    "lista_procesar=[\n",
    "    \"matriz_Train\",\n",
    "    \"matriz_Test\",\n",
    "    \"matriz_Val\"\n",
    "]\n",
    "\n",
    "str_info = str_info + \"\\n\\u25BA Obtención de las matrices \\u25C4\\n\"\n",
    "\n",
    "#Vamos procesando las matrices de una en una\n",
    "for element in lista_procesar:\n",
    "    if element in globals():\n",
    "        print(\"\\033[1m\" + str(element[7:])+ \"\\033[0m\")\n",
    "        str_info = str_info + str(element[7:]) +\"\\n\"\n",
    "        \n",
    "        #Si se trata del conjunto de entrenamiento sabemos que siempre tendremos una lista\n",
    "        if(\"Train\" in element):\n",
    "            globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_etiquetas\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_timestamp\"], globals()[\"listado_\"+'%s'%element[7:]] = Organizador_entrenamiento(globals()['%s'%element], globals()[\"secuencias_\"+'%s'%element[7:]], globals()[\"listado_base_\"+'%s'%element[7:]], etiquetas_juntas = globals()[\"junto_\"+'%s'%element[7:]], add_time=add_timestamp)\n",
    "        \n",
    "        #Si es el conjunto de testeo o validacion puede haber varios escenarios\n",
    "        else:\n",
    "            #Si tenemos una lista base le damos prioridad\n",
    "            if(\"listado_base_\"+ str(element[7:]) in globals()):\n",
    "                print(\"Matriz obtenida a partir de una lista base.\")\n",
    "                str_info = str_info + \"Matriz obtenida a partir de una lista base.\\n\"\n",
    "                globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_etiquetas\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_timestamp\"], globals()[\"listado_\"+'%s'%element[7:]] = Organizador_general(globals()['%s'%element], globals()[\"secuencias_\"+'%s'%element[7:]], globals()[\"listado_base_\"+'%s'%element[7:]], borrar_nuevos = globals()[\"borrar_datos_nuevos_\" +'%s'%element[7:]], etiquetas_juntas = globals()[\"junto_\"+'%s'%element[7:]], add_time=add_timestamp)\n",
    "            \n",
    "            #Si no tenemos lista base pero tenemos datos de entrenamiento lo lógico será que organizemos los datos siguiendo dicha lista    \n",
    "            elif(\"matriz_Train\" in globals()):\n",
    "                print(\"Matriz obtenida a partir de los datos de entrenamiento. Los AP's específicos de esta parte se encuentran al final\")\n",
    "                str_info = str_info + \"Matriz obtenida a partir de los datos de entrenamiento. Los AP's específicos de esta parte se encuentran al final.\\n\"\n",
    "                globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_etiquetas\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_timestamp\"], globals()[\"listado_\"+'%s'%element[7:]] = Organizador_general(globals()['%s'%element], globals()[\"secuencias_\"+'%s'%element[7:]], listado_base_Train, borrar_nuevos = globals()[\"borrar_datos_nuevos_\"+'%s'%element[7:]], etiquetas_juntas = globals()[\"junto_\"+'%s'%element[7:]], add_time=add_timestamp)\n",
    "            \n",
    "            #Si no estamos en ninguno de los casos anteriores no indicamos ningún orden\n",
    "            else:\n",
    "                print(\"Matriz obtenida a partir de los datos crudos sin ninguna referencia.\")\n",
    "                str_info = str_info + \"Matriz obtenida a partir de los datos crudos sin ninguna referencia.\\n\"\n",
    "                globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_etiquetas\"], globals()[\"matriz_\"+'%s'%element[7:]+\"_timestamp\"], globals()[\"listado_\"+'%s'%element[7:]] = Organizador_general(globals()['%s'%element], globals()[\"secuencias_\"+'%s'%element[7:]], etiquetas_juntas = globals()[\"junto_\"+'%s'%element[7:]], add_time=add_timestamp)\n",
    "        \n",
    "        print(\"Resultado de tamaño \"+str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"].shape[0])+ \"x\" +str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"].shape[1])+\".\\n Aquí un ejemplo de las primeras 10 filas y columnas:\\n\"+ str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"][:10,:10]))\n",
    "        str_info = str_info + \"Resultado de tamaño \"+str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"].shape[0])+ \"x\" +str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"].shape[1])+\".\\n Aquí un ejemplo de las primeras 10 filas y columnas:\\n\"+ str(globals()[\"matriz_\"+'%s'%element[7:]+\"_organizada\"][:10,:10]) + \"\\n\"\n",
    "\n",
    "#Recuento del tiempo\n",
    "tiempo_matriz = time.time()\n",
    "tiempo_total = tiempo_matriz-tiempo_inicio\n",
    "\n",
    "segundos=tiempo_total\n",
    " \n",
    "horas=int(segundos/3600)\n",
    "segundos-=horas*3600\n",
    "minutos=int(segundos/60)\n",
    "segundos-=int(minutos*60)\n",
    "segundos =int(segundos)\n",
    "\n",
    "print(\"\\n\\u23F3%s:%s:%s\" % (horas,minutos,segundos))\n",
    "str_info = str_info + \"\\n\\t\\u23F3 En total el procesado de la matriz ha tardado \" + str(horas) +\":\"+str(minutos)+\":\"+str(segundos)+\".\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f281c0",
   "metadata": {},
   "source": [
    "En el caso de que se haya procesado una matriz de entrenamiento se proporcionará feedback sobre el valor asignado a los puntos de acceso no visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3d797374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-11.0 -100.0\n",
      "Los valores que han aparecido en la matriz de entrenamiento son[-100.  -11.  -12.  -13.  -14.  -15.  -16.  -17.  -18.  -19.  -20.  -21.\n",
      "  -22.  -23.  -24.  -25.  -26.  -27.  -28.  -29.  -30.  -31.  -32.  -33.\n",
      "  -34.  -35.  -36.  -37.  -38.  -39.  -40.  -41.  -42.  -43.  -44.  -45.\n",
      "  -46.  -47.  -48.  -49.  -50.  -51.  -52.  -53.  -54.  -55.  -56.  -57.\n",
      "  -58.  -59.  -60.  -61.  -62.  -63.  -64.  -65.  -66.  -67.  -68.  -69.\n",
      "  -70.  -71.  -72.  -73.  -74.  -75.  -76.  -77.  -78.  -79.  -80.  -81.\n",
      "  -82.  -83.  -84.  -85.  -86.  -87.  -88.  -89.  -90.  -91.  -92.  -93.\n",
      "  -94.]\n",
      "Ningún valor es inferior al asignado.\n"
     ]
    }
   ],
   "source": [
    "if(check_minimun == True):\n",
    "    # Comprobamos los valores \n",
    "    if \"matriz_Train_organizada\" in globals():\n",
    "        if (junto_Train==True) & (add_timestamp==True):\n",
    "            valores = np.unique(matriz_Train_organizada[:,0:-3]).astype(float)\n",
    "        elif (junto_Train==True) & (add_timestamp==False):\n",
    "            valores = np.unique(matriz_Train_organizada[:,0:-2]).astype(float)\n",
    "        elif (junto_Train==False) & (add_timestamp==True):\n",
    "            valores = np.unique(matriz_Train_organizada[:,0:-1]).astype(float)\n",
    "        else:\n",
    "            valores = np.unique(matriz_Train_organizada).astype(float)\n",
    "\n",
    "        maximo = np.amax(valores)\n",
    "        minimo = np.amin(valores)\n",
    "        print(\"Los valores que han aparecido en la matriz de entrenamiento son\" + str(valores))\n",
    "        str_info = str_info + \"Los valores que han aparecido en la matriz de entrenamiento son\" + str(valores)+\"\\n\"\n",
    "\n",
    "        if(minimo < inv_value):\n",
    "            valores_menores = [valor for valor in valores if valor < inv_value]\n",
    "            val_y_frec = [str(\"Valor \" +str(valor)+\" aparece \"+str(np.count_nonzero(matriz_Train_organizada == valor))+\" veces.\") for valor in valores_menores]\n",
    "            print(\"\\t\\u2620Cuidado, has asignado un valor a los puntos de acceso no visbles que es menor que uno (o más) de los valores encontrados.\\nEsta es la lista de valores inferiores al asignado y la frecuencia con la que han aparecido cada uno de ellos: \"+str(val_y_frec))\n",
    "            str_info = str_info + \"\\t\\u2620Cuidado, has asignado un valor a los puntos de acceso no visbles que es menor que uno (o más) de los valores encontrados.\\nEsta es la lista de valores inferiores al asignado y la frecuencia con la que han aparecido cada uno de ellos: \"+str(val_y_frec) +\"\\n\"\n",
    "        else:\n",
    "            print(\"Ningún valor es inferior al asignado.\")\n",
    "            str_info = str_info + \"Ningún valor es inferior al asignado.\" + \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6debaf6e",
   "metadata": {},
   "source": [
    "## Escritura de los datos procesados\n",
    "\n",
    "Finalmente, una vez todos los datos han sido procesados los volvemos a meter a un archivo .csv que localizaremos en la carpeta \"Processed_data\". Dentro de dicha carpeta creamos otra con la fecha actual, sobre la cual crearemos distintas carpetas con el nombre de la hora en la que se ha guardado información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5020b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finalmente creamos creamos las carpetas donde guardarán los datos\n",
    "if(os.path.exists(date_path)!=True):\n",
    "    os.mkdir(date_path)\n",
    "    \n",
    "#Dentro de dicha carpeta creamos otra con la hora en la cual guardaremos los resultados\n",
    "os.mkdir(hour_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79aa95d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creacion de los indices de las filas\n",
    "#print(secuencias_Train)\n",
    "lista_indices=[\n",
    "    \"index_Train\",\n",
    "    \"index_Test\",\n",
    "    \"index_Val\"\n",
    "]\n",
    "\n",
    "for indice in lista_indices:\n",
    "    \n",
    "    if(\"secuencias_\"+indice[6:] in globals()):\n",
    "        #print(\"secuencias_\"+indice[6:])\n",
    "        globals()[\"index_\"+str(indice[6:])]=[]\n",
    "\n",
    "        for secuencia in globals()[\"secuencias_\"+indice[6:]]:\n",
    "            globals()[\"index_\"+str(indice[6:])] = globals()[\"index_\"+str(indice[6:])] + list(range(secuencia+1))\n",
    "\n",
    "    \n",
    "#print(len(index_Train), index_Train)\n",
    "#print(len(index_Test), index_Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1648a446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matriz_Train_organizada guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/matriz_Train_organizada.csv\n",
      "matriz_Test_organizada guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/matriz_Test_organizada.csv\n",
      "matriz_Val_organizada guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/matriz_Val_organizada.csv\n",
      "listado_Train guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/listado_Train.csv\n",
      "listado_Test guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/listado_Test.csv\n",
      "listado_Val guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/listado_Val.csv\n",
      "orden_Train guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/orden_Train.csv\n",
      "orden_Test guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/orden_Test.csv\n",
      "orden_Val guardada en /home/laura/Adrian/Procesaor_dataset/Database/Processed_data/15_12_2022/12:16:20/orden_Val.csv\n"
     ]
    }
   ],
   "source": [
    "#Pasamos cada matriz a csv y las guardamos en la carpeta.\n",
    "lista_matrices =[\n",
    "    \"matriz_Train_organizada\",\n",
    "    \"matriz_Train_etiquetas\",\n",
    "    \"matriz_Train_timestamp\",\n",
    "    \"matriz_Test_organizada\",\n",
    "    \"matriz_Test_etiquetas\",\n",
    "    \"matriz_Test_timestamp\",\n",
    "    \"matriz_Val_organizada\",\n",
    "    \"matriz_Val_etiquetas\",\n",
    "    \"matriz_Val_timestamp\",\n",
    "    \"listado_Train\",\n",
    "    \"listado_Test\",\n",
    "    \"listado_Val\",\n",
    "    \"orden_Train\",\n",
    "    \"orden_Test\",\n",
    "    \"orden_Val\"\n",
    "]\n",
    "\n",
    "for matriz in lista_matrices:    \n",
    "    #Comprobamos si la matriz existe\n",
    "    if (matriz in globals()):\n",
    "        #Si existe comprobamos si no está vacía\n",
    "        if(globals()['%s' % matriz] is not None):\n",
    "            file_path = hour_path + \"/\" + matriz + \".csv\"\n",
    "            \n",
    "            if(matriz[:7] == \"listado\"):\n",
    "                (pd.DataFrame(globals()['%s' % matriz])).to_csv(file_path, index=False, header=False)\n",
    "            \n",
    "            elif(matriz[-16:]==\"Train_organizada\"):\n",
    "                (pd.DataFrame(globals()['%s' % matriz], index = index_Train, columns= listado_Train)).to_csv(file_path)\n",
    "            \n",
    "            elif((\"etiquetas\" in matriz) or (\"timestamp\" in matriz)):\n",
    "                \n",
    "                if(\"etiquetas\" in matriz):\n",
    "                    col = [\"Latitud\", \"Longitud\"]\n",
    "                elif(\"timestamp\" in matriz):\n",
    "                    col = [\"Marca de tiempo\"]\n",
    "                    \n",
    "                if (\"Train\" in matriz):\n",
    "                    ind = index_Train\n",
    "                elif(\"Test\" in matriz):\n",
    "                    ind = index_Test\n",
    "                elif(\"Val\" in matriz):\n",
    "                    ind = index_Val\n",
    "                else:\n",
    "                    ind=False\n",
    "                \n",
    "                (pd.DataFrame(globals()['%s' % matriz],index = ind, columns = col)).to_csv(file_path)\n",
    "            \n",
    "            elif(\"Test_organizada\" in matriz):\n",
    "                (pd.DataFrame(globals()['%s' % matriz], index = index_Test, columns= listado_Test)).to_csv(file_path)\n",
    "            \n",
    "            elif(\"Val_organizada\" in matriz):\n",
    "                (pd.DataFrame(globals()['%s' % matriz], index = index_Val, columns= listado_Val)).to_csv(file_path)\n",
    "            \n",
    "            elif(\"orden\" in matriz):\n",
    "                (pd.DataFrame(globals()['%s' % matriz])).to_csv(file_path, index=False, header=False)\n",
    "                \n",
    "            else:\n",
    "                (pd.DataFrame(globals()['%s' % matriz])).to_csv(file_path, index=False)\n",
    "\n",
    "            #(pd.DataFrame(globals()['%s' % matriz])).to_csv(file_path, index=False)\n",
    "            print(str(matriz) + \" guardada en \" +file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e45e35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⏳0:2:21\n"
     ]
    }
   ],
   "source": [
    "#Recuento del tiempo\n",
    "tiempo_fin = time.time()\n",
    "tiempo_total = tiempo_fin-tiempo_inicio\n",
    "\n",
    "segundos=tiempo_total\n",
    " \n",
    "horas=int(segundos/3600)\n",
    "segundos-=horas*3600\n",
    "minutos=int(segundos/60)\n",
    "segundos-=int(minutos*60)\n",
    "segundos =int(segundos)\n",
    "\n",
    "print(\"\\n\\u23F3%s:%s:%s\" % (horas,minutos,segundos))\n",
    "str_info = str_info + \"\\u23F3 En total el programa ha tardado \" + str(horas) +\":\"+str(minutos)+\":\"+str(segundos)+\".\\n\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fa9e3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mPrograma finalizado con éxito\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Escribimos el .txt\n",
    "informacion = open(hour_path + \"/informacion.txt\", \"w\")\n",
    "informacion.write(str_info)\n",
    "informacion.close()\n",
    "\n",
    "#Acabamos el programa\n",
    "print(\"\\033[1mPrograma finalizado con éxito\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
